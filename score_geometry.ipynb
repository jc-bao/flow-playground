{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdb67b8",
   "metadata": {},
   "source": [
    "# Toy Experiment for Paper \"When Scores Learn Geometry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8fd4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaoyipan/miniforge3/envs/sam3d-objects/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, x_dim, t_dim, hidden_dim=128, num_layers=3):\n",
    "        in_dim = x_dim + t_dim\n",
    "        out_dim = x_dim\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xt, t):\n",
    "        return self.net(torch.cat([xt, t.unsqueeze(-1)], dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ab74a",
   "metadata": {},
   "source": [
    "Generate data from $x_0^2 + x_1^2 = 1$ manifold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c392e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaoyipan/miniforge3/envs/sam3d-objects/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m num_train_samples = \u001b[32m1000\u001b[39m\n\u001b[32m      2\u001b[39m batch_size = \u001b[32m128\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m theta = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m x = torch.stack([torch.cos(theta), torch.sin(theta)], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m dataset = TensorDataset(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = 1000\n",
    "batch_size = 128\n",
    "\n",
    "theta = torch.linspace(0, 2 * np.pi, num_train_samples, device=device)\n",
    "x = torch.stack([torch.cos(theta), torch.sin(theta)], dim=-1)\n",
    "\n",
    "dataset = TensorDataset(x)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f92ec",
   "metadata": {},
   "source": [
    "train velocity field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548624ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m(x_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, t_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "model = MLP(x_dim=2, t_dim=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "    epoch_loss_sum = 0.0\n",
    "    for x1, in dataloader:  # Unpack Tuple for x1\n",
    "        t = torch.rand((batch_size, ), device=device)\n",
    "        x0 = torch.randn((batch_size, 2), device=device)\n",
    "        xt = t * x1 + (1 - t) * x0\n",
    "        vel_target = (x1 - x0) / 1.0\n",
    "        vel_pred = model(xt, t)\n",
    "        loss = F.mse_loss(vel_pred, vel_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_sum += loss.item()\n",
    "    epoch_loss = epoch_loss_sum / len(dataloader)\n",
    "    tqdm.write(f\"Epoch {epoch} Loss: {epoch_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3d-objects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
